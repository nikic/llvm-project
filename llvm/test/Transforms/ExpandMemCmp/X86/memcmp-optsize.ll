; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 3
; RUN: opt -S -passes=expand-memcmp -mtriple=x86_64-unknown-unknown < %s | FileCheck %s --check-prefix=X64
; RUN: opt -S -passes=expand-memcmp -mtriple=x86_64-unknown-unknown -mattr=avx < %s | FileCheck %s --check-prefix=X64-AVX1
; RUN: opt -S -passes=expand-memcmp -mtriple=x86_64-unknown-unknown -mattr=avx2 < %s | FileCheck %s --check-prefix=X64-AVX2

; This tests codegen time inlining/optimization of memcmp
; rdar://6480398

@.str = private constant [65 x i8] c"0123456789012345678901234567890123456789012345678901234567890123\00", align 1

declare dso_local i32 @memcmp(ptr, ptr, i64)
declare dso_local i32 @bcmp(ptr, ptr, i64)

define i32 @length2(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length2(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0:[0-9]+]] {
; X64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = call i16 @llvm.bswap.i16(i16 [[TMP1]])
; X64-NEXT:    [[TMP4:%.*]] = call i16 @llvm.bswap.i16(i16 [[TMP2]])
; X64-NEXT:    [[TMP5:%.*]] = zext i16 [[TMP3]] to i32
; X64-NEXT:    [[TMP6:%.*]] = zext i16 [[TMP4]] to i32
; X64-NEXT:    [[TMP7:%.*]] = sub i32 [[TMP5]], [[TMP6]]
; X64-NEXT:    ret i32 [[TMP7]]
;
; X64-AVX1-LABEL: define i32 @length2(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1:[0-9]+]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = call i16 @llvm.bswap.i16(i16 [[TMP1]])
; X64-AVX1-NEXT:    [[TMP4:%.*]] = call i16 @llvm.bswap.i16(i16 [[TMP2]])
; X64-AVX1-NEXT:    [[TMP5:%.*]] = zext i16 [[TMP3]] to i32
; X64-AVX1-NEXT:    [[TMP6:%.*]] = zext i16 [[TMP4]] to i32
; X64-AVX1-NEXT:    [[TMP7:%.*]] = sub i32 [[TMP5]], [[TMP6]]
; X64-AVX1-NEXT:    ret i32 [[TMP7]]
;
; X64-AVX2-LABEL: define i32 @length2(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1:[0-9]+]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = call i16 @llvm.bswap.i16(i16 [[TMP1]])
; X64-AVX2-NEXT:    [[TMP4:%.*]] = call i16 @llvm.bswap.i16(i16 [[TMP2]])
; X64-AVX2-NEXT:    [[TMP5:%.*]] = zext i16 [[TMP3]] to i32
; X64-AVX2-NEXT:    [[TMP6:%.*]] = zext i16 [[TMP4]] to i32
; X64-AVX2-NEXT:    [[TMP7:%.*]] = sub i32 [[TMP5]], [[TMP6]]
; X64-AVX2-NEXT:    ret i32 [[TMP7]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 2) nounwind
  ret i32 %m
}

define i1 @length2_eq(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length2_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = icmp ne i16 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-NEXT:    ret i1 [[C]]
;
; X64-AVX1-LABEL: define i1 @length2_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = icmp ne i16 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX1-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-AVX1-NEXT:    ret i1 [[C]]
;
; X64-AVX2-LABEL: define i1 @length2_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = icmp ne i16 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX2-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-AVX2-NEXT:    ret i1 [[C]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 2) nounwind
  %c = icmp eq i32 %m, 0
  ret i1 %c
}

define i1 @length2_eq_const(ptr %X) nounwind optsize {
; X64-LABEL: define i1 @length2_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = icmp ne i16 [[TMP1]], 12849
; X64-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-NEXT:    ret i1 [[TMP2]]
;
; X64-AVX1-LABEL: define i1 @length2_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = icmp ne i16 [[TMP1]], 12849
; X64-AVX1-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP2]]
;
; X64-AVX2-LABEL: define i1 @length2_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = icmp ne i16 [[TMP1]], 12849
; X64-AVX2-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP2]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr getelementptr inbounds ([65 x i8], ptr @.str, i32 0, i32 1), i64 2) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i1 @length2_eq_nobuiltin_attr(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length2_eq_nobuiltin_attr(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 2) #[[ATTR2:[0-9]+]]
; X64-NEXT:    [[C:%.*]] = icmp eq i32 [[M]], 0
; X64-NEXT:    ret i1 [[C]]
;
; X64-AVX1-LABEL: define i1 @length2_eq_nobuiltin_attr(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 2) #[[ATTR3:[0-9]+]]
; X64-AVX1-NEXT:    [[C:%.*]] = icmp eq i32 [[M]], 0
; X64-AVX1-NEXT:    ret i1 [[C]]
;
; X64-AVX2-LABEL: define i1 @length2_eq_nobuiltin_attr(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 2) #[[ATTR3:[0-9]+]]
; X64-AVX2-NEXT:    [[C:%.*]] = icmp eq i32 [[M]], 0
; X64-AVX2-NEXT:    ret i1 [[C]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 2) nounwind nobuiltin
  %c = icmp eq i32 %m, 0
  ret i1 %c
}

define i32 @length3(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length3(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    br label [[LOADBB:%.*]]
; X64:       res_block:
; X64-NEXT:    [[TMP1:%.*]] = icmp ult i16 [[TMP5:%.*]], [[TMP6:%.*]]
; X64-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-NEXT:    br label [[ENDBLOCK:%.*]]
; X64:       loadbb:
; X64-NEXT:    [[TMP3:%.*]] = load i16, ptr [[X]], align 1
; X64-NEXT:    [[TMP4:%.*]] = load i16, ptr [[Y]], align 1
; X64-NEXT:    [[TMP5]] = call i16 @llvm.bswap.i16(i16 [[TMP3]])
; X64-NEXT:    [[TMP6]] = call i16 @llvm.bswap.i16(i16 [[TMP4]])
; X64-NEXT:    [[TMP7:%.*]] = icmp eq i16 [[TMP5]], [[TMP6]]
; X64-NEXT:    br i1 [[TMP7]], label [[LOADBB1:%.*]], label [[RES_BLOCK:%.*]]
; X64:       loadbb1:
; X64-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 2
; X64-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 2
; X64-NEXT:    [[TMP10:%.*]] = load i8, ptr [[TMP8]], align 1
; X64-NEXT:    [[TMP11:%.*]] = load i8, ptr [[TMP9]], align 1
; X64-NEXT:    [[TMP12:%.*]] = zext i8 [[TMP10]] to i32
; X64-NEXT:    [[TMP13:%.*]] = zext i8 [[TMP11]] to i32
; X64-NEXT:    [[TMP14:%.*]] = sub i32 [[TMP12]], [[TMP13]]
; X64-NEXT:    br label [[ENDBLOCK]]
; X64:       endblock:
; X64-NEXT:    [[PHI_RES:%.*]] = phi i32 [ [[TMP14]], [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX1-LABEL: define i32 @length3(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX1:       res_block:
; X64-AVX1-NEXT:    [[TMP1:%.*]] = icmp ult i16 [[TMP5:%.*]], [[TMP6:%.*]]
; X64-AVX1-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX1-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX1:       loadbb:
; X64-AVX1-NEXT:    [[TMP3:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP4:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP5]] = call i16 @llvm.bswap.i16(i16 [[TMP3]])
; X64-AVX1-NEXT:    [[TMP6]] = call i16 @llvm.bswap.i16(i16 [[TMP4]])
; X64-AVX1-NEXT:    [[TMP7:%.*]] = icmp eq i16 [[TMP5]], [[TMP6]]
; X64-AVX1-NEXT:    br i1 [[TMP7]], label [[LOADBB1:%.*]], label [[RES_BLOCK:%.*]]
; X64-AVX1:       loadbb1:
; X64-AVX1-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 2
; X64-AVX1-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 2
; X64-AVX1-NEXT:    [[TMP10:%.*]] = load i8, ptr [[TMP8]], align 1
; X64-AVX1-NEXT:    [[TMP11:%.*]] = load i8, ptr [[TMP9]], align 1
; X64-AVX1-NEXT:    [[TMP12:%.*]] = zext i8 [[TMP10]] to i32
; X64-AVX1-NEXT:    [[TMP13:%.*]] = zext i8 [[TMP11]] to i32
; X64-AVX1-NEXT:    [[TMP14:%.*]] = sub i32 [[TMP12]], [[TMP13]]
; X64-AVX1-NEXT:    br label [[ENDBLOCK]]
; X64-AVX1:       endblock:
; X64-AVX1-NEXT:    [[PHI_RES:%.*]] = phi i32 [ [[TMP14]], [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX1-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX2-LABEL: define i32 @length3(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX2:       res_block:
; X64-AVX2-NEXT:    [[TMP1:%.*]] = icmp ult i16 [[TMP5:%.*]], [[TMP6:%.*]]
; X64-AVX2-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX2-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX2:       loadbb:
; X64-AVX2-NEXT:    [[TMP3:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP4:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP5]] = call i16 @llvm.bswap.i16(i16 [[TMP3]])
; X64-AVX2-NEXT:    [[TMP6]] = call i16 @llvm.bswap.i16(i16 [[TMP4]])
; X64-AVX2-NEXT:    [[TMP7:%.*]] = icmp eq i16 [[TMP5]], [[TMP6]]
; X64-AVX2-NEXT:    br i1 [[TMP7]], label [[LOADBB1:%.*]], label [[RES_BLOCK:%.*]]
; X64-AVX2:       loadbb1:
; X64-AVX2-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 2
; X64-AVX2-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 2
; X64-AVX2-NEXT:    [[TMP10:%.*]] = load i8, ptr [[TMP8]], align 1
; X64-AVX2-NEXT:    [[TMP11:%.*]] = load i8, ptr [[TMP9]], align 1
; X64-AVX2-NEXT:    [[TMP12:%.*]] = zext i8 [[TMP10]] to i32
; X64-AVX2-NEXT:    [[TMP13:%.*]] = zext i8 [[TMP11]] to i32
; X64-AVX2-NEXT:    [[TMP14:%.*]] = sub i32 [[TMP12]], [[TMP13]]
; X64-AVX2-NEXT:    br label [[ENDBLOCK]]
; X64-AVX2:       endblock:
; X64-AVX2-NEXT:    [[PHI_RES:%.*]] = phi i32 [ [[TMP14]], [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX2-NEXT:    ret i32 [[PHI_RES]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 3) nounwind
  ret i32 %m
}

define i1 @length3_eq(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length3_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = xor i16 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 2
; X64-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 2
; X64-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP4]], align 1
; X64-NEXT:    [[TMP7:%.*]] = load i8, ptr [[TMP5]], align 1
; X64-NEXT:    [[TMP8:%.*]] = zext i8 [[TMP6]] to i16
; X64-NEXT:    [[TMP9:%.*]] = zext i8 [[TMP7]] to i16
; X64-NEXT:    [[TMP10:%.*]] = xor i16 [[TMP8]], [[TMP9]]
; X64-NEXT:    [[TMP11:%.*]] = or i16 [[TMP3]], [[TMP10]]
; X64-NEXT:    [[TMP12:%.*]] = icmp ne i16 [[TMP11]], 0
; X64-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-NEXT:    ret i1 [[TMP12]]
;
; X64-AVX1-LABEL: define i1 @length3_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = xor i16 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 2
; X64-AVX1-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 2
; X64-AVX1-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP4]], align 1
; X64-AVX1-NEXT:    [[TMP7:%.*]] = load i8, ptr [[TMP5]], align 1
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i8 [[TMP6]] to i16
; X64-AVX1-NEXT:    [[TMP9:%.*]] = zext i8 [[TMP7]] to i16
; X64-AVX1-NEXT:    [[TMP10:%.*]] = xor i16 [[TMP8]], [[TMP9]]
; X64-AVX1-NEXT:    [[TMP11:%.*]] = or i16 [[TMP3]], [[TMP10]]
; X64-AVX1-NEXT:    [[TMP12:%.*]] = icmp ne i16 [[TMP11]], 0
; X64-AVX1-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP12]]
;
; X64-AVX2-LABEL: define i1 @length3_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = xor i16 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 2
; X64-AVX2-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 2
; X64-AVX2-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP4]], align 1
; X64-AVX2-NEXT:    [[TMP7:%.*]] = load i8, ptr [[TMP5]], align 1
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i8 [[TMP6]] to i16
; X64-AVX2-NEXT:    [[TMP9:%.*]] = zext i8 [[TMP7]] to i16
; X64-AVX2-NEXT:    [[TMP10:%.*]] = xor i16 [[TMP8]], [[TMP9]]
; X64-AVX2-NEXT:    [[TMP11:%.*]] = or i16 [[TMP3]], [[TMP10]]
; X64-AVX2-NEXT:    [[TMP12:%.*]] = icmp ne i16 [[TMP11]], 0
; X64-AVX2-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP12]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 3) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i32 @length4(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length4(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP1]])
; X64-NEXT:    [[TMP4:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP2]])
; X64-NEXT:    [[TMP5:%.*]] = icmp ugt i32 [[TMP3]], [[TMP4]]
; X64-NEXT:    [[TMP6:%.*]] = icmp ult i32 [[TMP3]], [[TMP4]]
; X64-NEXT:    [[TMP7:%.*]] = zext i1 [[TMP5]] to i32
; X64-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP6]] to i32
; X64-NEXT:    [[TMP9:%.*]] = sub i32 [[TMP7]], [[TMP8]]
; X64-NEXT:    ret i32 [[TMP9]]
;
; X64-AVX1-LABEL: define i32 @length4(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP1]])
; X64-AVX1-NEXT:    [[TMP4:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP2]])
; X64-AVX1-NEXT:    [[TMP5:%.*]] = icmp ugt i32 [[TMP3]], [[TMP4]]
; X64-AVX1-NEXT:    [[TMP6:%.*]] = icmp ult i32 [[TMP3]], [[TMP4]]
; X64-AVX1-NEXT:    [[TMP7:%.*]] = zext i1 [[TMP5]] to i32
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP6]] to i32
; X64-AVX1-NEXT:    [[TMP9:%.*]] = sub i32 [[TMP7]], [[TMP8]]
; X64-AVX1-NEXT:    ret i32 [[TMP9]]
;
; X64-AVX2-LABEL: define i32 @length4(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP1]])
; X64-AVX2-NEXT:    [[TMP4:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP2]])
; X64-AVX2-NEXT:    [[TMP5:%.*]] = icmp ugt i32 [[TMP3]], [[TMP4]]
; X64-AVX2-NEXT:    [[TMP6:%.*]] = icmp ult i32 [[TMP3]], [[TMP4]]
; X64-AVX2-NEXT:    [[TMP7:%.*]] = zext i1 [[TMP5]] to i32
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP6]] to i32
; X64-AVX2-NEXT:    [[TMP9:%.*]] = sub i32 [[TMP7]], [[TMP8]]
; X64-AVX2-NEXT:    ret i32 [[TMP9]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 4) nounwind
  ret i32 %m
}

define i1 @length4_eq(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length4_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = icmp ne i32 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-NEXT:    ret i1 [[TMP3]]
;
; X64-AVX1-LABEL: define i1 @length4_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = icmp ne i32 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP3]]
;
; X64-AVX2-LABEL: define i1 @length4_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = icmp ne i32 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP3]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 4) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i1 @length4_eq_const(ptr %X) nounwind optsize {
; X64-LABEL: define i1 @length4_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = icmp ne i32 [[TMP1]], 875770417
; X64-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP3]], 0
; X64-NEXT:    ret i1 [[C]]
;
; X64-AVX1-LABEL: define i1 @length4_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = icmp ne i32 [[TMP1]], 875770417
; X64-AVX1-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX1-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP3]], 0
; X64-AVX1-NEXT:    ret i1 [[C]]
;
; X64-AVX2-LABEL: define i1 @length4_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = icmp ne i32 [[TMP1]], 875770417
; X64-AVX2-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX2-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP3]], 0
; X64-AVX2-NEXT:    ret i1 [[C]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr getelementptr inbounds ([65 x i8], ptr @.str, i32 0, i32 1), i64 4) nounwind
  %c = icmp eq i32 %m, 0
  ret i1 %c
}

define i32 @length5(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length5(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    br label [[LOADBB:%.*]]
; X64:       res_block:
; X64-NEXT:    [[TMP1:%.*]] = icmp ult i32 [[TMP5:%.*]], [[TMP6:%.*]]
; X64-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-NEXT:    br label [[ENDBLOCK:%.*]]
; X64:       loadbb:
; X64-NEXT:    [[TMP3:%.*]] = load i32, ptr [[X]], align 1
; X64-NEXT:    [[TMP4:%.*]] = load i32, ptr [[Y]], align 1
; X64-NEXT:    [[TMP5]] = call i32 @llvm.bswap.i32(i32 [[TMP3]])
; X64-NEXT:    [[TMP6]] = call i32 @llvm.bswap.i32(i32 [[TMP4]])
; X64-NEXT:    [[TMP7:%.*]] = icmp eq i32 [[TMP5]], [[TMP6]]
; X64-NEXT:    br i1 [[TMP7]], label [[LOADBB1:%.*]], label [[RES_BLOCK:%.*]]
; X64:       loadbb1:
; X64-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 4
; X64-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 4
; X64-NEXT:    [[TMP10:%.*]] = load i8, ptr [[TMP8]], align 1
; X64-NEXT:    [[TMP11:%.*]] = load i8, ptr [[TMP9]], align 1
; X64-NEXT:    [[TMP12:%.*]] = zext i8 [[TMP10]] to i32
; X64-NEXT:    [[TMP13:%.*]] = zext i8 [[TMP11]] to i32
; X64-NEXT:    [[TMP14:%.*]] = sub i32 [[TMP12]], [[TMP13]]
; X64-NEXT:    br label [[ENDBLOCK]]
; X64:       endblock:
; X64-NEXT:    [[PHI_RES:%.*]] = phi i32 [ [[TMP14]], [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX1-LABEL: define i32 @length5(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX1:       res_block:
; X64-AVX1-NEXT:    [[TMP1:%.*]] = icmp ult i32 [[TMP5:%.*]], [[TMP6:%.*]]
; X64-AVX1-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX1-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX1:       loadbb:
; X64-AVX1-NEXT:    [[TMP3:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP4:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP5]] = call i32 @llvm.bswap.i32(i32 [[TMP3]])
; X64-AVX1-NEXT:    [[TMP6]] = call i32 @llvm.bswap.i32(i32 [[TMP4]])
; X64-AVX1-NEXT:    [[TMP7:%.*]] = icmp eq i32 [[TMP5]], [[TMP6]]
; X64-AVX1-NEXT:    br i1 [[TMP7]], label [[LOADBB1:%.*]], label [[RES_BLOCK:%.*]]
; X64-AVX1:       loadbb1:
; X64-AVX1-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 4
; X64-AVX1-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 4
; X64-AVX1-NEXT:    [[TMP10:%.*]] = load i8, ptr [[TMP8]], align 1
; X64-AVX1-NEXT:    [[TMP11:%.*]] = load i8, ptr [[TMP9]], align 1
; X64-AVX1-NEXT:    [[TMP12:%.*]] = zext i8 [[TMP10]] to i32
; X64-AVX1-NEXT:    [[TMP13:%.*]] = zext i8 [[TMP11]] to i32
; X64-AVX1-NEXT:    [[TMP14:%.*]] = sub i32 [[TMP12]], [[TMP13]]
; X64-AVX1-NEXT:    br label [[ENDBLOCK]]
; X64-AVX1:       endblock:
; X64-AVX1-NEXT:    [[PHI_RES:%.*]] = phi i32 [ [[TMP14]], [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX1-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX2-LABEL: define i32 @length5(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX2:       res_block:
; X64-AVX2-NEXT:    [[TMP1:%.*]] = icmp ult i32 [[TMP5:%.*]], [[TMP6:%.*]]
; X64-AVX2-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX2-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX2:       loadbb:
; X64-AVX2-NEXT:    [[TMP3:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP4:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP5]] = call i32 @llvm.bswap.i32(i32 [[TMP3]])
; X64-AVX2-NEXT:    [[TMP6]] = call i32 @llvm.bswap.i32(i32 [[TMP4]])
; X64-AVX2-NEXT:    [[TMP7:%.*]] = icmp eq i32 [[TMP5]], [[TMP6]]
; X64-AVX2-NEXT:    br i1 [[TMP7]], label [[LOADBB1:%.*]], label [[RES_BLOCK:%.*]]
; X64-AVX2:       loadbb1:
; X64-AVX2-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 4
; X64-AVX2-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 4
; X64-AVX2-NEXT:    [[TMP10:%.*]] = load i8, ptr [[TMP8]], align 1
; X64-AVX2-NEXT:    [[TMP11:%.*]] = load i8, ptr [[TMP9]], align 1
; X64-AVX2-NEXT:    [[TMP12:%.*]] = zext i8 [[TMP10]] to i32
; X64-AVX2-NEXT:    [[TMP13:%.*]] = zext i8 [[TMP11]] to i32
; X64-AVX2-NEXT:    [[TMP14:%.*]] = sub i32 [[TMP12]], [[TMP13]]
; X64-AVX2-NEXT:    br label [[ENDBLOCK]]
; X64-AVX2:       endblock:
; X64-AVX2-NEXT:    [[PHI_RES:%.*]] = phi i32 [ [[TMP14]], [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX2-NEXT:    ret i32 [[PHI_RES]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 5) nounwind
  ret i32 %m
}

define i1 @length5_eq(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length5_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = xor i32 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 4
; X64-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 4
; X64-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP4]], align 1
; X64-NEXT:    [[TMP7:%.*]] = load i8, ptr [[TMP5]], align 1
; X64-NEXT:    [[TMP8:%.*]] = zext i8 [[TMP6]] to i32
; X64-NEXT:    [[TMP9:%.*]] = zext i8 [[TMP7]] to i32
; X64-NEXT:    [[TMP10:%.*]] = xor i32 [[TMP8]], [[TMP9]]
; X64-NEXT:    [[TMP11:%.*]] = or i32 [[TMP3]], [[TMP10]]
; X64-NEXT:    [[TMP12:%.*]] = icmp ne i32 [[TMP11]], 0
; X64-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-NEXT:    ret i1 [[TMP12]]
;
; X64-AVX1-LABEL: define i1 @length5_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = xor i32 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 4
; X64-AVX1-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 4
; X64-AVX1-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP4]], align 1
; X64-AVX1-NEXT:    [[TMP7:%.*]] = load i8, ptr [[TMP5]], align 1
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i8 [[TMP6]] to i32
; X64-AVX1-NEXT:    [[TMP9:%.*]] = zext i8 [[TMP7]] to i32
; X64-AVX1-NEXT:    [[TMP10:%.*]] = xor i32 [[TMP8]], [[TMP9]]
; X64-AVX1-NEXT:    [[TMP11:%.*]] = or i32 [[TMP3]], [[TMP10]]
; X64-AVX1-NEXT:    [[TMP12:%.*]] = icmp ne i32 [[TMP11]], 0
; X64-AVX1-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP12]]
;
; X64-AVX2-LABEL: define i1 @length5_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i32, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i32, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = xor i32 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 4
; X64-AVX2-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 4
; X64-AVX2-NEXT:    [[TMP6:%.*]] = load i8, ptr [[TMP4]], align 1
; X64-AVX2-NEXT:    [[TMP7:%.*]] = load i8, ptr [[TMP5]], align 1
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i8 [[TMP6]] to i32
; X64-AVX2-NEXT:    [[TMP9:%.*]] = zext i8 [[TMP7]] to i32
; X64-AVX2-NEXT:    [[TMP10:%.*]] = xor i32 [[TMP8]], [[TMP9]]
; X64-AVX2-NEXT:    [[TMP11:%.*]] = or i32 [[TMP3]], [[TMP10]]
; X64-AVX2-NEXT:    [[TMP12:%.*]] = icmp ne i32 [[TMP11]], 0
; X64-AVX2-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP12]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 5) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i32 @length8(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length8(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = call i64 @llvm.bswap.i64(i64 [[TMP1]])
; X64-NEXT:    [[TMP4:%.*]] = call i64 @llvm.bswap.i64(i64 [[TMP2]])
; X64-NEXT:    [[TMP5:%.*]] = icmp ugt i64 [[TMP3]], [[TMP4]]
; X64-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[TMP3]], [[TMP4]]
; X64-NEXT:    [[TMP7:%.*]] = zext i1 [[TMP5]] to i32
; X64-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP6]] to i32
; X64-NEXT:    [[TMP9:%.*]] = sub i32 [[TMP7]], [[TMP8]]
; X64-NEXT:    ret i32 [[TMP9]]
;
; X64-AVX1-LABEL: define i32 @length8(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = call i64 @llvm.bswap.i64(i64 [[TMP1]])
; X64-AVX1-NEXT:    [[TMP4:%.*]] = call i64 @llvm.bswap.i64(i64 [[TMP2]])
; X64-AVX1-NEXT:    [[TMP5:%.*]] = icmp ugt i64 [[TMP3]], [[TMP4]]
; X64-AVX1-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[TMP3]], [[TMP4]]
; X64-AVX1-NEXT:    [[TMP7:%.*]] = zext i1 [[TMP5]] to i32
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP6]] to i32
; X64-AVX1-NEXT:    [[TMP9:%.*]] = sub i32 [[TMP7]], [[TMP8]]
; X64-AVX1-NEXT:    ret i32 [[TMP9]]
;
; X64-AVX2-LABEL: define i32 @length8(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = call i64 @llvm.bswap.i64(i64 [[TMP1]])
; X64-AVX2-NEXT:    [[TMP4:%.*]] = call i64 @llvm.bswap.i64(i64 [[TMP2]])
; X64-AVX2-NEXT:    [[TMP5:%.*]] = icmp ugt i64 [[TMP3]], [[TMP4]]
; X64-AVX2-NEXT:    [[TMP6:%.*]] = icmp ult i64 [[TMP3]], [[TMP4]]
; X64-AVX2-NEXT:    [[TMP7:%.*]] = zext i1 [[TMP5]] to i32
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP6]] to i32
; X64-AVX2-NEXT:    [[TMP9:%.*]] = sub i32 [[TMP7]], [[TMP8]]
; X64-AVX2-NEXT:    ret i32 [[TMP9]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 8) nounwind
  ret i32 %m
}

define i1 @length8_eq(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length8_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = icmp ne i64 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-NEXT:    ret i1 [[C]]
;
; X64-AVX1-LABEL: define i1 @length8_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = icmp ne i64 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX1-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-AVX1-NEXT:    ret i1 [[C]]
;
; X64-AVX2-LABEL: define i1 @length8_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = icmp ne i64 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX2-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-AVX2-NEXT:    ret i1 [[C]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 8) nounwind
  %c = icmp eq i32 %m, 0
  ret i1 %c
}

define i1 @length8_eq_const(ptr %X) nounwind optsize {
; X64-LABEL: define i1 @length8_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = icmp ne i64 [[TMP1]], 3978425819141910832
; X64-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-NEXT:    ret i1 [[TMP2]]
;
; X64-AVX1-LABEL: define i1 @length8_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = icmp ne i64 [[TMP1]], 3978425819141910832
; X64-AVX1-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP2]]
;
; X64-AVX2-LABEL: define i1 @length8_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = icmp ne i64 [[TMP1]], 3978425819141910832
; X64-AVX2-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP2]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr @.str, i64 8) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i1 @length12_eq(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i1 @length12_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = xor i64 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-NEXT:    [[TMP6:%.*]] = load i32, ptr [[TMP4]], align 1
; X64-NEXT:    [[TMP7:%.*]] = load i32, ptr [[TMP5]], align 1
; X64-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP6]] to i64
; X64-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP7]] to i64
; X64-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP8]], [[TMP9]]
; X64-NEXT:    [[TMP11:%.*]] = or i64 [[TMP3]], [[TMP10]]
; X64-NEXT:    [[TMP12:%.*]] = icmp ne i64 [[TMP11]], 0
; X64-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-NEXT:    ret i1 [[TMP12]]
;
; X64-AVX1-LABEL: define i1 @length12_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = xor i64 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-AVX1-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-AVX1-NEXT:    [[TMP6:%.*]] = load i32, ptr [[TMP4]], align 1
; X64-AVX1-NEXT:    [[TMP7:%.*]] = load i32, ptr [[TMP5]], align 1
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP6]] to i64
; X64-AVX1-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP7]] to i64
; X64-AVX1-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP8]], [[TMP9]]
; X64-AVX1-NEXT:    [[TMP11:%.*]] = or i64 [[TMP3]], [[TMP10]]
; X64-AVX1-NEXT:    [[TMP12:%.*]] = icmp ne i64 [[TMP11]], 0
; X64-AVX1-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP12]]
;
; X64-AVX2-LABEL: define i1 @length12_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = xor i64 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-AVX2-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-AVX2-NEXT:    [[TMP6:%.*]] = load i32, ptr [[TMP4]], align 1
; X64-AVX2-NEXT:    [[TMP7:%.*]] = load i32, ptr [[TMP5]], align 1
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP6]] to i64
; X64-AVX2-NEXT:    [[TMP9:%.*]] = zext i32 [[TMP7]] to i64
; X64-AVX2-NEXT:    [[TMP10:%.*]] = xor i64 [[TMP8]], [[TMP9]]
; X64-AVX2-NEXT:    [[TMP11:%.*]] = or i64 [[TMP3]], [[TMP10]]
; X64-AVX2-NEXT:    [[TMP12:%.*]] = icmp ne i64 [[TMP11]], 0
; X64-AVX2-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP12]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 12) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i32 @length12(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length12(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    br label [[LOADBB:%.*]]
; X64:       res_block:
; X64-NEXT:    [[PHI_SRC1:%.*]] = phi i64 [ [[TMP5:%.*]], [[LOADBB]] ], [ [[TMP14:%.*]], [[LOADBB1:%.*]] ]
; X64-NEXT:    [[PHI_SRC2:%.*]] = phi i64 [ [[TMP6:%.*]], [[LOADBB]] ], [ [[TMP15:%.*]], [[LOADBB1]] ]
; X64-NEXT:    [[TMP1:%.*]] = icmp ult i64 [[PHI_SRC1]], [[PHI_SRC2]]
; X64-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-NEXT:    br label [[ENDBLOCK:%.*]]
; X64:       loadbb:
; X64-NEXT:    [[TMP3:%.*]] = load i64, ptr [[X]], align 1
; X64-NEXT:    [[TMP4:%.*]] = load i64, ptr [[Y]], align 1
; X64-NEXT:    [[TMP5]] = call i64 @llvm.bswap.i64(i64 [[TMP3]])
; X64-NEXT:    [[TMP6]] = call i64 @llvm.bswap.i64(i64 [[TMP4]])
; X64-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[TMP5]], [[TMP6]]
; X64-NEXT:    br i1 [[TMP7]], label [[LOADBB1]], label [[RES_BLOCK:%.*]]
; X64:       loadbb1:
; X64-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-NEXT:    [[TMP10:%.*]] = load i32, ptr [[TMP8]], align 1
; X64-NEXT:    [[TMP11:%.*]] = load i32, ptr [[TMP9]], align 1
; X64-NEXT:    [[TMP12:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP10]])
; X64-NEXT:    [[TMP13:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP11]])
; X64-NEXT:    [[TMP14]] = zext i32 [[TMP12]] to i64
; X64-NEXT:    [[TMP15]] = zext i32 [[TMP13]] to i64
; X64-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[TMP14]], [[TMP15]]
; X64-NEXT:    br i1 [[TMP16]], label [[ENDBLOCK]], label [[RES_BLOCK]]
; X64:       endblock:
; X64-NEXT:    [[PHI_RES:%.*]] = phi i32 [ 0, [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX1-LABEL: define i32 @length12(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX1:       res_block:
; X64-AVX1-NEXT:    [[PHI_SRC1:%.*]] = phi i64 [ [[TMP5:%.*]], [[LOADBB]] ], [ [[TMP14:%.*]], [[LOADBB1:%.*]] ]
; X64-AVX1-NEXT:    [[PHI_SRC2:%.*]] = phi i64 [ [[TMP6:%.*]], [[LOADBB]] ], [ [[TMP15:%.*]], [[LOADBB1]] ]
; X64-AVX1-NEXT:    [[TMP1:%.*]] = icmp ult i64 [[PHI_SRC1]], [[PHI_SRC2]]
; X64-AVX1-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX1-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX1:       loadbb:
; X64-AVX1-NEXT:    [[TMP3:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP4:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP5]] = call i64 @llvm.bswap.i64(i64 [[TMP3]])
; X64-AVX1-NEXT:    [[TMP6]] = call i64 @llvm.bswap.i64(i64 [[TMP4]])
; X64-AVX1-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[TMP5]], [[TMP6]]
; X64-AVX1-NEXT:    br i1 [[TMP7]], label [[LOADBB1]], label [[RES_BLOCK:%.*]]
; X64-AVX1:       loadbb1:
; X64-AVX1-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-AVX1-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-AVX1-NEXT:    [[TMP10:%.*]] = load i32, ptr [[TMP8]], align 1
; X64-AVX1-NEXT:    [[TMP11:%.*]] = load i32, ptr [[TMP9]], align 1
; X64-AVX1-NEXT:    [[TMP12:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP10]])
; X64-AVX1-NEXT:    [[TMP13:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP11]])
; X64-AVX1-NEXT:    [[TMP14]] = zext i32 [[TMP12]] to i64
; X64-AVX1-NEXT:    [[TMP15]] = zext i32 [[TMP13]] to i64
; X64-AVX1-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[TMP14]], [[TMP15]]
; X64-AVX1-NEXT:    br i1 [[TMP16]], label [[ENDBLOCK]], label [[RES_BLOCK]]
; X64-AVX1:       endblock:
; X64-AVX1-NEXT:    [[PHI_RES:%.*]] = phi i32 [ 0, [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX1-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX2-LABEL: define i32 @length12(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX2:       res_block:
; X64-AVX2-NEXT:    [[PHI_SRC1:%.*]] = phi i64 [ [[TMP5:%.*]], [[LOADBB]] ], [ [[TMP14:%.*]], [[LOADBB1:%.*]] ]
; X64-AVX2-NEXT:    [[PHI_SRC2:%.*]] = phi i64 [ [[TMP6:%.*]], [[LOADBB]] ], [ [[TMP15:%.*]], [[LOADBB1]] ]
; X64-AVX2-NEXT:    [[TMP1:%.*]] = icmp ult i64 [[PHI_SRC1]], [[PHI_SRC2]]
; X64-AVX2-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX2-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX2:       loadbb:
; X64-AVX2-NEXT:    [[TMP3:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP4:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP5]] = call i64 @llvm.bswap.i64(i64 [[TMP3]])
; X64-AVX2-NEXT:    [[TMP6]] = call i64 @llvm.bswap.i64(i64 [[TMP4]])
; X64-AVX2-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[TMP5]], [[TMP6]]
; X64-AVX2-NEXT:    br i1 [[TMP7]], label [[LOADBB1]], label [[RES_BLOCK:%.*]]
; X64-AVX2:       loadbb1:
; X64-AVX2-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-AVX2-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-AVX2-NEXT:    [[TMP10:%.*]] = load i32, ptr [[TMP8]], align 1
; X64-AVX2-NEXT:    [[TMP11:%.*]] = load i32, ptr [[TMP9]], align 1
; X64-AVX2-NEXT:    [[TMP12:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP10]])
; X64-AVX2-NEXT:    [[TMP13:%.*]] = call i32 @llvm.bswap.i32(i32 [[TMP11]])
; X64-AVX2-NEXT:    [[TMP14]] = zext i32 [[TMP12]] to i64
; X64-AVX2-NEXT:    [[TMP15]] = zext i32 [[TMP13]] to i64
; X64-AVX2-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[TMP14]], [[TMP15]]
; X64-AVX2-NEXT:    br i1 [[TMP16]], label [[ENDBLOCK]], label [[RES_BLOCK]]
; X64-AVX2:       endblock:
; X64-AVX2-NEXT:    [[PHI_RES:%.*]] = phi i32 [ 0, [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX2-NEXT:    ret i32 [[PHI_RES]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 12) nounwind
  ret i32 %m
}

; PR33329 - https://bugs.llvm.org/show_bug.cgi?id=33329

define i32 @length16(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length16(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    br label [[LOADBB:%.*]]
; X64:       res_block:
; X64-NEXT:    [[PHI_SRC1:%.*]] = phi i64 [ [[TMP5:%.*]], [[LOADBB]] ], [ [[TMP12:%.*]], [[LOADBB1:%.*]] ]
; X64-NEXT:    [[PHI_SRC2:%.*]] = phi i64 [ [[TMP6:%.*]], [[LOADBB]] ], [ [[TMP13:%.*]], [[LOADBB1]] ]
; X64-NEXT:    [[TMP1:%.*]] = icmp ult i64 [[PHI_SRC1]], [[PHI_SRC2]]
; X64-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-NEXT:    br label [[ENDBLOCK:%.*]]
; X64:       loadbb:
; X64-NEXT:    [[TMP3:%.*]] = load i64, ptr [[X]], align 1
; X64-NEXT:    [[TMP4:%.*]] = load i64, ptr [[Y]], align 1
; X64-NEXT:    [[TMP5]] = call i64 @llvm.bswap.i64(i64 [[TMP3]])
; X64-NEXT:    [[TMP6]] = call i64 @llvm.bswap.i64(i64 [[TMP4]])
; X64-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[TMP5]], [[TMP6]]
; X64-NEXT:    br i1 [[TMP7]], label [[LOADBB1]], label [[RES_BLOCK:%.*]]
; X64:       loadbb1:
; X64-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-NEXT:    [[TMP10:%.*]] = load i64, ptr [[TMP8]], align 1
; X64-NEXT:    [[TMP11:%.*]] = load i64, ptr [[TMP9]], align 1
; X64-NEXT:    [[TMP12]] = call i64 @llvm.bswap.i64(i64 [[TMP10]])
; X64-NEXT:    [[TMP13]] = call i64 @llvm.bswap.i64(i64 [[TMP11]])
; X64-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[TMP12]], [[TMP13]]
; X64-NEXT:    br i1 [[TMP14]], label [[ENDBLOCK]], label [[RES_BLOCK]]
; X64:       endblock:
; X64-NEXT:    [[PHI_RES:%.*]] = phi i32 [ 0, [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX1-LABEL: define i32 @length16(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX1:       res_block:
; X64-AVX1-NEXT:    [[PHI_SRC1:%.*]] = phi i64 [ [[TMP5:%.*]], [[LOADBB]] ], [ [[TMP12:%.*]], [[LOADBB1:%.*]] ]
; X64-AVX1-NEXT:    [[PHI_SRC2:%.*]] = phi i64 [ [[TMP6:%.*]], [[LOADBB]] ], [ [[TMP13:%.*]], [[LOADBB1]] ]
; X64-AVX1-NEXT:    [[TMP1:%.*]] = icmp ult i64 [[PHI_SRC1]], [[PHI_SRC2]]
; X64-AVX1-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX1-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX1:       loadbb:
; X64-AVX1-NEXT:    [[TMP3:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP4:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP5]] = call i64 @llvm.bswap.i64(i64 [[TMP3]])
; X64-AVX1-NEXT:    [[TMP6]] = call i64 @llvm.bswap.i64(i64 [[TMP4]])
; X64-AVX1-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[TMP5]], [[TMP6]]
; X64-AVX1-NEXT:    br i1 [[TMP7]], label [[LOADBB1]], label [[RES_BLOCK:%.*]]
; X64-AVX1:       loadbb1:
; X64-AVX1-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-AVX1-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-AVX1-NEXT:    [[TMP10:%.*]] = load i64, ptr [[TMP8]], align 1
; X64-AVX1-NEXT:    [[TMP11:%.*]] = load i64, ptr [[TMP9]], align 1
; X64-AVX1-NEXT:    [[TMP12]] = call i64 @llvm.bswap.i64(i64 [[TMP10]])
; X64-AVX1-NEXT:    [[TMP13]] = call i64 @llvm.bswap.i64(i64 [[TMP11]])
; X64-AVX1-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[TMP12]], [[TMP13]]
; X64-AVX1-NEXT:    br i1 [[TMP14]], label [[ENDBLOCK]], label [[RES_BLOCK]]
; X64-AVX1:       endblock:
; X64-AVX1-NEXT:    [[PHI_RES:%.*]] = phi i32 [ 0, [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX1-NEXT:    ret i32 [[PHI_RES]]
;
; X64-AVX2-LABEL: define i32 @length16(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    br label [[LOADBB:%.*]]
; X64-AVX2:       res_block:
; X64-AVX2-NEXT:    [[PHI_SRC1:%.*]] = phi i64 [ [[TMP5:%.*]], [[LOADBB]] ], [ [[TMP12:%.*]], [[LOADBB1:%.*]] ]
; X64-AVX2-NEXT:    [[PHI_SRC2:%.*]] = phi i64 [ [[TMP6:%.*]], [[LOADBB]] ], [ [[TMP13:%.*]], [[LOADBB1]] ]
; X64-AVX2-NEXT:    [[TMP1:%.*]] = icmp ult i64 [[PHI_SRC1]], [[PHI_SRC2]]
; X64-AVX2-NEXT:    [[TMP2:%.*]] = select i1 [[TMP1]], i32 -1, i32 1
; X64-AVX2-NEXT:    br label [[ENDBLOCK:%.*]]
; X64-AVX2:       loadbb:
; X64-AVX2-NEXT:    [[TMP3:%.*]] = load i64, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP4:%.*]] = load i64, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP5]] = call i64 @llvm.bswap.i64(i64 [[TMP3]])
; X64-AVX2-NEXT:    [[TMP6]] = call i64 @llvm.bswap.i64(i64 [[TMP4]])
; X64-AVX2-NEXT:    [[TMP7:%.*]] = icmp eq i64 [[TMP5]], [[TMP6]]
; X64-AVX2-NEXT:    br i1 [[TMP7]], label [[LOADBB1]], label [[RES_BLOCK:%.*]]
; X64-AVX2:       loadbb1:
; X64-AVX2-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[X]], i64 8
; X64-AVX2-NEXT:    [[TMP9:%.*]] = getelementptr i8, ptr [[Y]], i64 8
; X64-AVX2-NEXT:    [[TMP10:%.*]] = load i64, ptr [[TMP8]], align 1
; X64-AVX2-NEXT:    [[TMP11:%.*]] = load i64, ptr [[TMP9]], align 1
; X64-AVX2-NEXT:    [[TMP12]] = call i64 @llvm.bswap.i64(i64 [[TMP10]])
; X64-AVX2-NEXT:    [[TMP13]] = call i64 @llvm.bswap.i64(i64 [[TMP11]])
; X64-AVX2-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[TMP12]], [[TMP13]]
; X64-AVX2-NEXT:    br i1 [[TMP14]], label [[ENDBLOCK]], label [[RES_BLOCK]]
; X64-AVX2:       endblock:
; X64-AVX2-NEXT:    [[PHI_RES:%.*]] = phi i32 [ 0, [[LOADBB1]] ], [ [[TMP2]], [[RES_BLOCK]] ]
; X64-AVX2-NEXT:    ret i32 [[PHI_RES]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 16) nounwind
  ret i32 %m
}

define i1 @length16_eq(ptr %x, ptr %y) nounwind optsize {
; X64-SSE2-LABEL: length16_eq:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    movdqu (%rdi), %xmm0
; X64-SSE2-NEXT:    movdqu (%rsi), %xmm1
; X64-SSE2-NEXT:    pcmpeqb %xmm0, %xmm1
; X64-SSE2-NEXT:    pmovmskb %xmm1, %eax
; X64-SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; X64-SSE2-NEXT:    setne %al
; X64-SSE2-NEXT:    retq
;
; X64-AVX-LABEL: length16_eq:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vmovdqu (%rdi), %xmm0
; X64-AVX-NEXT:    vpxor (%rsi), %xmm0, %xmm0
; X64-AVX-NEXT:    vptest %xmm0, %xmm0
; X64-AVX-NEXT:    setne %al
; X64-AVX-NEXT:    retq
; X64-LABEL: define i1 @length16_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = icmp ne i128 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-NEXT:    ret i1 [[TMP3]]
;
; X64-AVX1-LABEL: define i1 @length16_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = icmp ne i128 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP3]]
;
; X64-AVX2-LABEL: define i1 @length16_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = icmp ne i128 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP3]]
;
  %call = tail call i32 @memcmp(ptr %x, ptr %y, i64 16) nounwind
  %cmp = icmp ne i32 %call, 0
  ret i1 %cmp
}

define i1 @length16_eq_const(ptr %X) nounwind optsize {
; X64-SSE2-LABEL: length16_eq_const:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    movdqu (%rdi), %xmm0
; X64-SSE2-NEXT:    pcmpeqb {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; X64-SSE2-NEXT:    pmovmskb %xmm0, %eax
; X64-SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; X64-SSE2-NEXT:    sete %al
; X64-SSE2-NEXT:    retq
;
; X64-AVX-LABEL: length16_eq_const:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vmovdqu (%rdi), %xmm0
; X64-AVX-NEXT:    vpxor {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; X64-AVX-NEXT:    vptest %xmm0, %xmm0
; X64-AVX-NEXT:    sete %al
; X64-AVX-NEXT:    retq
; X64-LABEL: define i1 @length16_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = icmp ne i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP3]], 0
; X64-NEXT:    ret i1 [[C]]
;
; X64-AVX1-LABEL: define i1 @length16_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = icmp ne i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-AVX1-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX1-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP3]], 0
; X64-AVX1-NEXT:    ret i1 [[C]]
;
; X64-AVX2-LABEL: define i1 @length16_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = icmp ne i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-AVX2-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX2-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP3]], 0
; X64-AVX2-NEXT:    ret i1 [[C]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr @.str, i64 16) nounwind
  %c = icmp eq i32 %m, 0
  ret i1 %c
}

; PR33914 - https://bugs.llvm.org/show_bug.cgi?id=33914

define i32 @length24(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length24(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 24) #[[ATTR3:[0-9]+]]
; X64-NEXT:    ret i32 [[M]]
;
; X64-AVX1-LABEL: define i32 @length24(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 24) #[[ATTR4:[0-9]+]]
; X64-AVX1-NEXT:    ret i32 [[M]]
;
; X64-AVX2-LABEL: define i32 @length24(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 24) #[[ATTR4:[0-9]+]]
; X64-AVX2-NEXT:    ret i32 [[M]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 24) nounwind
  ret i32 %m
}

define i1 @length24_eq(ptr %x, ptr %y) nounwind optsize {
; X64-SSE2-LABEL: length24_eq:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    movdqu (%rdi), %xmm0
; X64-SSE2-NEXT:    movdqu (%rsi), %xmm1
; X64-SSE2-NEXT:    pcmpeqb %xmm0, %xmm1
; X64-SSE2-NEXT:    movq {{.*#+}} xmm0 = mem[0],zero
; X64-SSE2-NEXT:    movq {{.*#+}} xmm2 = mem[0],zero
; X64-SSE2-NEXT:    pcmpeqb %xmm0, %xmm2
; X64-SSE2-NEXT:    pand %xmm1, %xmm2
; X64-SSE2-NEXT:    pmovmskb %xmm2, %eax
; X64-SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; X64-SSE2-NEXT:    sete %al
; X64-SSE2-NEXT:    retq
;
; X64-AVX-LABEL: length24_eq:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vmovdqu (%rdi), %xmm0
; X64-AVX-NEXT:    vmovq {{.*#+}} xmm1 = mem[0],zero
; X64-AVX-NEXT:    vmovq {{.*#+}} xmm2 = mem[0],zero
; X64-AVX-NEXT:    vpxor %xmm2, %xmm1, %xmm1
; X64-AVX-NEXT:    vpxor (%rsi), %xmm0, %xmm0
; X64-AVX-NEXT:    vpor %xmm0, %xmm1, %xmm0
; X64-AVX-NEXT:    vptest %xmm0, %xmm0
; X64-AVX-NEXT:    sete %al
; X64-AVX-NEXT:    retq
; X64-LABEL: define i1 @length24_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = xor i128 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 16
; X64-NEXT:    [[TMP6:%.*]] = load i64, ptr [[TMP4]], align 1
; X64-NEXT:    [[TMP7:%.*]] = load i64, ptr [[TMP5]], align 1
; X64-NEXT:    [[TMP8:%.*]] = zext i64 [[TMP6]] to i128
; X64-NEXT:    [[TMP9:%.*]] = zext i64 [[TMP7]] to i128
; X64-NEXT:    [[TMP10:%.*]] = xor i128 [[TMP8]], [[TMP9]]
; X64-NEXT:    [[TMP11:%.*]] = or i128 [[TMP3]], [[TMP10]]
; X64-NEXT:    [[TMP12:%.*]] = icmp ne i128 [[TMP11]], 0
; X64-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-NEXT:    [[CMP:%.*]] = icmp eq i32 [[TMP13]], 0
; X64-NEXT:    ret i1 [[CMP]]
;
; X64-AVX1-LABEL: define i1 @length24_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = xor i128 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-AVX1-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 16
; X64-AVX1-NEXT:    [[TMP6:%.*]] = load i64, ptr [[TMP4]], align 1
; X64-AVX1-NEXT:    [[TMP7:%.*]] = load i64, ptr [[TMP5]], align 1
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i64 [[TMP6]] to i128
; X64-AVX1-NEXT:    [[TMP9:%.*]] = zext i64 [[TMP7]] to i128
; X64-AVX1-NEXT:    [[TMP10:%.*]] = xor i128 [[TMP8]], [[TMP9]]
; X64-AVX1-NEXT:    [[TMP11:%.*]] = or i128 [[TMP3]], [[TMP10]]
; X64-AVX1-NEXT:    [[TMP12:%.*]] = icmp ne i128 [[TMP11]], 0
; X64-AVX1-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX1-NEXT:    [[CMP:%.*]] = icmp eq i32 [[TMP13]], 0
; X64-AVX1-NEXT:    ret i1 [[CMP]]
;
; X64-AVX2-LABEL: define i1 @length24_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = xor i128 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-AVX2-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 16
; X64-AVX2-NEXT:    [[TMP6:%.*]] = load i64, ptr [[TMP4]], align 1
; X64-AVX2-NEXT:    [[TMP7:%.*]] = load i64, ptr [[TMP5]], align 1
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i64 [[TMP6]] to i128
; X64-AVX2-NEXT:    [[TMP9:%.*]] = zext i64 [[TMP7]] to i128
; X64-AVX2-NEXT:    [[TMP10:%.*]] = xor i128 [[TMP8]], [[TMP9]]
; X64-AVX2-NEXT:    [[TMP11:%.*]] = or i128 [[TMP3]], [[TMP10]]
; X64-AVX2-NEXT:    [[TMP12:%.*]] = icmp ne i128 [[TMP11]], 0
; X64-AVX2-NEXT:    [[TMP13:%.*]] = zext i1 [[TMP12]] to i32
; X64-AVX2-NEXT:    [[CMP:%.*]] = icmp eq i32 [[TMP13]], 0
; X64-AVX2-NEXT:    ret i1 [[CMP]]
;
  %call = tail call i32 @memcmp(ptr %x, ptr %y, i64 24) nounwind
  %cmp = icmp eq i32 %call, 0
  ret i1 %cmp
}

define i1 @length24_eq_const(ptr %X) nounwind optsize {
; X64-SSE2-LABEL: length24_eq_const:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    movdqu (%rdi), %xmm0
; X64-SSE2-NEXT:    movq {{.*#+}} xmm1 = mem[0],zero
; X64-SSE2-NEXT:    pcmpeqb {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1
; X64-SSE2-NEXT:    pcmpeqb {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; X64-SSE2-NEXT:    pand %xmm1, %xmm0
; X64-SSE2-NEXT:    pmovmskb %xmm0, %eax
; X64-SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; X64-SSE2-NEXT:    setne %al
; X64-SSE2-NEXT:    retq
;
; X64-AVX-LABEL: length24_eq_const:
; X64-AVX:       # %bb.0:
; X64-AVX-NEXT:    vmovdqu (%rdi), %xmm0
; X64-AVX-NEXT:    vmovq {{.*#+}} xmm1 = mem[0],zero
; X64-AVX-NEXT:    vpxor {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1, %xmm1
; X64-AVX-NEXT:    vpxor {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; X64-AVX-NEXT:    vpor %xmm1, %xmm0, %xmm0
; X64-AVX-NEXT:    vptest %xmm0, %xmm0
; X64-AVX-NEXT:    setne %al
; X64-AVX-NEXT:    retq
; X64-LABEL: define i1 @length24_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = xor i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-NEXT:    [[TMP4:%.*]] = load i64, ptr [[TMP3]], align 1
; X64-NEXT:    [[TMP5:%.*]] = zext i64 [[TMP4]] to i128
; X64-NEXT:    [[TMP6:%.*]] = xor i128 [[TMP5]], 3689065127958034230
; X64-NEXT:    [[TMP7:%.*]] = or i128 [[TMP2]], [[TMP6]]
; X64-NEXT:    [[TMP8:%.*]] = icmp ne i128 [[TMP7]], 0
; X64-NEXT:    [[TMP9:%.*]] = zext i1 [[TMP8]] to i32
; X64-NEXT:    ret i1 [[TMP8]]
;
; X64-AVX1-LABEL: define i1 @length24_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = xor i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-AVX1-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-AVX1-NEXT:    [[TMP4:%.*]] = load i64, ptr [[TMP3]], align 1
; X64-AVX1-NEXT:    [[TMP5:%.*]] = zext i64 [[TMP4]] to i128
; X64-AVX1-NEXT:    [[TMP6:%.*]] = xor i128 [[TMP5]], 3689065127958034230
; X64-AVX1-NEXT:    [[TMP7:%.*]] = or i128 [[TMP2]], [[TMP6]]
; X64-AVX1-NEXT:    [[TMP8:%.*]] = icmp ne i128 [[TMP7]], 0
; X64-AVX1-NEXT:    [[TMP9:%.*]] = zext i1 [[TMP8]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP8]]
;
; X64-AVX2-LABEL: define i1 @length24_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = xor i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-AVX2-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-AVX2-NEXT:    [[TMP4:%.*]] = load i64, ptr [[TMP3]], align 1
; X64-AVX2-NEXT:    [[TMP5:%.*]] = zext i64 [[TMP4]] to i128
; X64-AVX2-NEXT:    [[TMP6:%.*]] = xor i128 [[TMP5]], 3689065127958034230
; X64-AVX2-NEXT:    [[TMP7:%.*]] = or i128 [[TMP2]], [[TMP6]]
; X64-AVX2-NEXT:    [[TMP8:%.*]] = icmp ne i128 [[TMP7]], 0
; X64-AVX2-NEXT:    [[TMP9:%.*]] = zext i1 [[TMP8]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP8]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr @.str, i64 24) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i32 @length32(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length32(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 32) #[[ATTR3]]
; X64-NEXT:    ret i32 [[M]]
;
; X64-AVX1-LABEL: define i32 @length32(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 32) #[[ATTR4]]
; X64-AVX1-NEXT:    ret i32 [[M]]
;
; X64-AVX2-LABEL: define i32 @length32(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 32) #[[ATTR4]]
; X64-AVX2-NEXT:    ret i32 [[M]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 32) nounwind
  ret i32 %m
}

; PR33325 - https://bugs.llvm.org/show_bug.cgi?id=33325

define i1 @length32_eq(ptr %x, ptr %y) nounwind optsize {
; X64-SSE2-LABEL: length32_eq:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    movdqu (%rdi), %xmm0
; X64-SSE2-NEXT:    movdqu 16(%rdi), %xmm1
; X64-SSE2-NEXT:    movdqu (%rsi), %xmm2
; X64-SSE2-NEXT:    pcmpeqb %xmm0, %xmm2
; X64-SSE2-NEXT:    movdqu 16(%rsi), %xmm0
; X64-SSE2-NEXT:    pcmpeqb %xmm1, %xmm0
; X64-SSE2-NEXT:    pand %xmm2, %xmm0
; X64-SSE2-NEXT:    pmovmskb %xmm0, %eax
; X64-SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; X64-SSE2-NEXT:    sete %al
; X64-SSE2-NEXT:    retq
;
; X64-LABEL: define i1 @length32_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i128, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = xor i128 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 16
; X64-NEXT:    [[TMP6:%.*]] = load i128, ptr [[TMP4]], align 1
; X64-NEXT:    [[TMP7:%.*]] = load i128, ptr [[TMP5]], align 1
; X64-NEXT:    [[TMP8:%.*]] = xor i128 [[TMP6]], [[TMP7]]
; X64-NEXT:    [[TMP9:%.*]] = or i128 [[TMP3]], [[TMP8]]
; X64-NEXT:    [[TMP10:%.*]] = icmp ne i128 [[TMP9]], 0
; X64-NEXT:    [[TMP11:%.*]] = zext i1 [[TMP10]] to i32
; X64-NEXT:    [[CMP:%.*]] = icmp eq i32 [[TMP11]], 0
; X64-NEXT:    ret i1 [[CMP]]
;
; X64-AVX1-LABEL: define i1 @length32_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i256, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = icmp ne i256 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX1-NEXT:    [[CMP:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-AVX1-NEXT:    ret i1 [[CMP]]
;
; X64-AVX2-LABEL: define i1 @length32_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i256, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = icmp ne i256 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX2-NEXT:    [[CMP:%.*]] = icmp eq i32 [[TMP4]], 0
; X64-AVX2-NEXT:    ret i1 [[CMP]]
;
  %call = tail call i32 @memcmp(ptr %x, ptr %y, i64 32) nounwind
  %cmp = icmp eq i32 %call, 0
  ret i1 %cmp
}

define i1 @length32_eq_const(ptr %X) nounwind optsize {
; X64-SSE2-LABEL: length32_eq_const:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    movdqu (%rdi), %xmm0
; X64-SSE2-NEXT:    movdqu 16(%rdi), %xmm1
; X64-SSE2-NEXT:    pcmpeqb {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1
; X64-SSE2-NEXT:    pcmpeqb {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; X64-SSE2-NEXT:    pand %xmm1, %xmm0
; X64-SSE2-NEXT:    pmovmskb %xmm0, %eax
; X64-SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; X64-SSE2-NEXT:    setne %al
; X64-SSE2-NEXT:    retq
;
; X64-LABEL: define i1 @length32_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i128, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = xor i128 [[TMP1]], 70720121592765328381466889075544961328
; X64-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[X]], i64 16
; X64-NEXT:    [[TMP4:%.*]] = load i128, ptr [[TMP3]], align 1
; X64-NEXT:    [[TMP5:%.*]] = xor i128 [[TMP4]], 65382562593882267225249597816672106294
; X64-NEXT:    [[TMP6:%.*]] = or i128 [[TMP2]], [[TMP5]]
; X64-NEXT:    [[TMP7:%.*]] = icmp ne i128 [[TMP6]], 0
; X64-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP7]] to i32
; X64-NEXT:    ret i1 [[TMP7]]
;
; X64-AVX1-LABEL: define i1 @length32_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = icmp ne i256 [[TMP1]], 22248533154802671749360035741805466271990224543450513484713781259640245465392
; X64-AVX1-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP2]]
;
; X64-AVX2-LABEL: define i1 @length32_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = icmp ne i256 [[TMP1]], 22248533154802671749360035741805466271990224543450513484713781259640245465392
; X64-AVX2-NEXT:    [[TMP3:%.*]] = zext i1 [[TMP2]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP2]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr @.str, i64 32) nounwind
  %c = icmp ne i32 %m, 0
  ret i1 %c
}

define i32 @length64(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @length64(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 64) #[[ATTR3]]
; X64-NEXT:    ret i32 [[M]]
;
; X64-AVX1-LABEL: define i32 @length64(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 64) #[[ATTR4]]
; X64-AVX1-NEXT:    ret i32 [[M]]
;
; X64-AVX2-LABEL: define i32 @length64(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 64) #[[ATTR4]]
; X64-AVX2-NEXT:    ret i32 [[M]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr %Y, i64 64) nounwind
  ret i32 %m
}

define i1 @length64_eq(ptr %x, ptr %y) nounwind optsize {
; X64-SSE2-LABEL: length64_eq:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    pushq %rax
; X64-SSE2-NEXT:    movl $64, %edx
; X64-SSE2-NEXT:    callq memcmp
; X64-SSE2-NEXT:    testl %eax, %eax
; X64-SSE2-NEXT:    setne %al
; X64-SSE2-NEXT:    popq %rcx
; X64-SSE2-NEXT:    retq
;
; X64-LABEL: define i1 @length64_eq(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[CALL:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr [[Y]], i64 64) #[[ATTR3]]
; X64-NEXT:    [[CMP:%.*]] = icmp ne i32 [[CALL]], 0
; X64-NEXT:    ret i1 [[CMP]]
;
; X64-AVX1-LABEL: define i1 @length64_eq(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i256, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = xor i256 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 32
; X64-AVX1-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 32
; X64-AVX1-NEXT:    [[TMP6:%.*]] = load i256, ptr [[TMP4]], align 1
; X64-AVX1-NEXT:    [[TMP7:%.*]] = load i256, ptr [[TMP5]], align 1
; X64-AVX1-NEXT:    [[TMP8:%.*]] = xor i256 [[TMP6]], [[TMP7]]
; X64-AVX1-NEXT:    [[TMP9:%.*]] = or i256 [[TMP3]], [[TMP8]]
; X64-AVX1-NEXT:    [[TMP10:%.*]] = icmp ne i256 [[TMP9]], 0
; X64-AVX1-NEXT:    [[TMP11:%.*]] = zext i1 [[TMP10]] to i32
; X64-AVX1-NEXT:    ret i1 [[TMP10]]
;
; X64-AVX2-LABEL: define i1 @length64_eq(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i256, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = xor i256 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr [[X]], i64 32
; X64-AVX2-NEXT:    [[TMP5:%.*]] = getelementptr i8, ptr [[Y]], i64 32
; X64-AVX2-NEXT:    [[TMP6:%.*]] = load i256, ptr [[TMP4]], align 1
; X64-AVX2-NEXT:    [[TMP7:%.*]] = load i256, ptr [[TMP5]], align 1
; X64-AVX2-NEXT:    [[TMP8:%.*]] = xor i256 [[TMP6]], [[TMP7]]
; X64-AVX2-NEXT:    [[TMP9:%.*]] = or i256 [[TMP3]], [[TMP8]]
; X64-AVX2-NEXT:    [[TMP10:%.*]] = icmp ne i256 [[TMP9]], 0
; X64-AVX2-NEXT:    [[TMP11:%.*]] = zext i1 [[TMP10]] to i32
; X64-AVX2-NEXT:    ret i1 [[TMP10]]
;
  %call = tail call i32 @memcmp(ptr %x, ptr %y, i64 64) nounwind
  %cmp = icmp ne i32 %call, 0
  ret i1 %cmp
}

define i1 @length64_eq_const(ptr %X) nounwind optsize {
; X64-SSE2-LABEL: length64_eq_const:
; X64-SSE2:       # %bb.0:
; X64-SSE2-NEXT:    pushq %rax
; X64-SSE2-NEXT:    movl $.L.str, %esi
; X64-SSE2-NEXT:    movl $64, %edx
; X64-SSE2-NEXT:    callq memcmp
; X64-SSE2-NEXT:    testl %eax, %eax
; X64-SSE2-NEXT:    sete %al
; X64-SSE2-NEXT:    popq %rcx
; X64-SSE2-NEXT:    retq
;
; X64-LABEL: define i1 @length64_eq_const(
; X64-SAME: ptr [[X:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[M:%.*]] = tail call i32 @memcmp(ptr [[X]], ptr @.str, i64 64) #[[ATTR3]]
; X64-NEXT:    [[C:%.*]] = icmp eq i32 [[M]], 0
; X64-NEXT:    ret i1 [[C]]
;
; X64-AVX1-LABEL: define i1 @length64_eq_const(
; X64-AVX1-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = xor i256 [[TMP1]], 22248533154802671749360035741805466271990224543450513484713781259640245465392
; X64-AVX1-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[X]], i64 32
; X64-AVX1-NEXT:    [[TMP4:%.*]] = load i256, ptr [[TMP3]], align 1
; X64-AVX1-NEXT:    [[TMP5:%.*]] = xor i256 [[TMP4]], 23156637116659864195145731957391441738757757709540232586892941433547502400306
; X64-AVX1-NEXT:    [[TMP6:%.*]] = or i256 [[TMP2]], [[TMP5]]
; X64-AVX1-NEXT:    [[TMP7:%.*]] = icmp ne i256 [[TMP6]], 0
; X64-AVX1-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP7]] to i32
; X64-AVX1-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP8]], 0
; X64-AVX1-NEXT:    ret i1 [[C]]
;
; X64-AVX2-LABEL: define i1 @length64_eq_const(
; X64-AVX2-SAME: ptr [[X:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i256, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = xor i256 [[TMP1]], 22248533154802671749360035741805466271990224543450513484713781259640245465392
; X64-AVX2-NEXT:    [[TMP3:%.*]] = getelementptr i8, ptr [[X]], i64 32
; X64-AVX2-NEXT:    [[TMP4:%.*]] = load i256, ptr [[TMP3]], align 1
; X64-AVX2-NEXT:    [[TMP5:%.*]] = xor i256 [[TMP4]], 23156637116659864195145731957391441738757757709540232586892941433547502400306
; X64-AVX2-NEXT:    [[TMP6:%.*]] = or i256 [[TMP2]], [[TMP5]]
; X64-AVX2-NEXT:    [[TMP7:%.*]] = icmp ne i256 [[TMP6]], 0
; X64-AVX2-NEXT:    [[TMP8:%.*]] = zext i1 [[TMP7]] to i32
; X64-AVX2-NEXT:    [[C:%.*]] = icmp eq i32 [[TMP8]], 0
; X64-AVX2-NEXT:    ret i1 [[C]]
;
  %m = tail call i32 @memcmp(ptr %X, ptr @.str, i64 64) nounwind
  %c = icmp eq i32 %m, 0
  ret i1 %c
}

define i32 @bcmp_length2(ptr %X, ptr %Y) nounwind optsize {
; X64-LABEL: define i32 @bcmp_length2(
; X64-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR0]] {
; X64-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-NEXT:    [[TMP3:%.*]] = icmp ne i16 [[TMP1]], [[TMP2]]
; X64-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-NEXT:    ret i32 [[TMP4]]
;
; X64-AVX1-LABEL: define i32 @bcmp_length2(
; X64-AVX1-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX1-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX1-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX1-NEXT:    [[TMP3:%.*]] = icmp ne i16 [[TMP1]], [[TMP2]]
; X64-AVX1-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX1-NEXT:    ret i32 [[TMP4]]
;
; X64-AVX2-LABEL: define i32 @bcmp_length2(
; X64-AVX2-SAME: ptr [[X:%.*]], ptr [[Y:%.*]]) #[[ATTR1]] {
; X64-AVX2-NEXT:    [[TMP1:%.*]] = load i16, ptr [[X]], align 1
; X64-AVX2-NEXT:    [[TMP2:%.*]] = load i16, ptr [[Y]], align 1
; X64-AVX2-NEXT:    [[TMP3:%.*]] = icmp ne i16 [[TMP1]], [[TMP2]]
; X64-AVX2-NEXT:    [[TMP4:%.*]] = zext i1 [[TMP3]] to i32
; X64-AVX2-NEXT:    ret i32 [[TMP4]]
;
  %m = tail call i32 @bcmp(ptr %X, ptr %Y, i64 2) nounwind
  ret i32 %m
}
